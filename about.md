# Abnormal Respiratory Sound Detector

## Overview

This project aims to detect the presence of crackles and wheezes in respiratory audio recordings using an LSTM model. It includes scripts for data preparation, model training, a backend API (Flask) to serve the model, and a web-based frontend UI for user interaction, audio upload, prediction visualization, and dataset insights.

**Note:** This project analyzes *respiratory sounds* (crackles, wheezes), not *heartbeat sounds*.

## Features

*   **Audio Upload:** Users can upload respiratory audio files (e.g., `.wav`).
*   **LSTM Prediction:** Analyzes audio segments using a trained LSTM model to predict the probability of crackles and wheezes.
*   **Waveform Visualization:** Displays the uploaded audio waveform using WaveSurfer.js.
*   **Prediction Overlay (Wow Factor 1):** Visualizes detected abnormal segments (crackles/wheezes above a threshold) directly onto the waveform.
*   **Confidence Scores (Wow Factor 2):** Shows the model's confidence (probability) for crackle and wheeze predictions per segment.
*   **Dataset Insights (Wow Factor 3):** Displays charts (using Chart.js) showing the prevalence of abnormalities based on chest location and recording equipment from the original dataset, providing context.
*   **Modular Structure:** Separates data processing, model training, backend logic, and frontend presentation.

## Directory Structure
abnormal-respiratory-sound-detector/
├── data/
│ └── raw/ # Original, unmodified dataset files
│ ├── audio_and_txt_files/ # Contains all .wav and corresponding .txt annotation files
│ │ ├── 101_1b1_Al_sc_Meditron.wav
│ │ ├── 101_1b1_Al_sc_Meditron.txt
│ │ └── ... (all other audio/text pairs)
│ └── train_test.txt # File defining train/test split based on recordings
│
├── scripts/ # Scripts for one-time or development tasks
│ ├── prepare_data.py # Processes raw data into features for the model
│ └── train_model.py # Trains the LSTM model
│
├── saved_assets/ # Files generated by scripts, used by the backend
│ ├── preprocessed_data.npz # (Optional) Saved numpy array of features/labels after prep
│ ├── scaler.joblib # Saved feature scaler object
│ └── best_model.keras # Saved trained Keras LSTM model
│
├── static/ # Static files for the web UI (CSS, JS, potentially images/libs)
│ ├── css/
│ │ └── style.css # Stylesheet for the frontend
│ ├── js/
│ │ └── main.js # Frontend JavaScript logic (upload, fetch, visualization)
│ └── libs/ # (Optional) Local copies of libraries like WaveSurfer, Chart.js
│
├── templates/ # HTML templates (rendered by Flask)
│ └── index.html # Main HTML page for the UI
│
├── app.py # Flask backend application file
├── requirements.txt # Python dependencies for the project
└── README.md # This file


## File Descriptions

*   **`data/raw/audio_and_txt_files/`**: Contains the original audio (`.wav`) and annotation (`.txt`) files provided in the dataset.
*   **`data/raw/train_test.txt`**: Defines which recording belongs to the training set and which belongs to the test set. Used by `prepare_data.py`.
*   **`scripts/prepare_data.py`**:
    *   **Input:** Reads `.wav` files, `.txt` annotation files, and `train_test.txt` from `data/raw/`.
    *   **Function:**
        1.  Parses filenames for metadata.
        2.  Loads audio and annotation data.
        3.  Segments audio based on respiratory cycle annotations.
        4.  Extracts MFCC features from each segment.
        5.  Pads/truncates sequences to a fixed length (`MAX_SEQ_LENGTH`).
        6.  Splits data into training and testing sets based on `train_test.txt`.
        7.  Fits a `StandardScaler` on the training features and scales both training and testing features.
    *   **Output:** Saves the `scaler.joblib` object and optionally `preprocessed_data.npz` (containing scaled `X_train`, `y_train`, `X_test`, `y_test`) into the `saved_assets/` directory.
*   **`scripts/train_model.py`**:
    *   **Input:** Loads the preprocessed and scaled data (e.g., from `preprocessed_data.npz` or directly uses variables if run sequentially after `prepare_data.py`). Requires `MAX_SEQ_LENGTH` and `N_MFCC` constants defined consistently with `prepare_data.py`.
    *   **Function:**
        1.  Defines the LSTM model architecture (using TensorFlow/Keras).
        2.  Compiles the model (optimizer, loss function like `binary_crossentropy`, metrics).
        3.  Trains the model using the training data (`X_train_scaled`, `y_train`).
        4.  Uses validation data (`X_test_scaled`, `y_test`) for monitoring and callbacks (like `ModelCheckpoint`, `EarlyStopping`).
        5.  Evaluates the best model on the test set.
    *   **Output:** Saves the best trained model (`best_model.keras`) to the `saved_assets/` directory.
*   **`saved_assets/preprocessed_data.npz`**: (Optional) A NumPy archive file containing the split, scaled, and padded feature arrays (`X_train`, `X_test`) and label arrays (`y_train`, `y_test`). Useful for debugging or re-running training without reprocessing.
*   **`saved_assets/scaler.joblib`**: The `StandardScaler` object fitted on the training data. **Crucial** for scaling new, unseen audio data in the backend (`app.py`) exactly like the training data was scaled.
*   **`saved_assets/best_model.keras`**: The trained LSTM model file saved by Keras. This contains the model architecture and learned weights needed for making predictions.
*   **`static/css/style.css`**: Contains CSS rules to style the `index.html` page, including layout, colors, waveform appearance, chart containers, etc.
*   **`static/js/main.js`**:
    *   Handles frontend interactions.
    *   Manages file input changes.
    *   Initializes WaveSurfer.js to load and display the audio waveform.
    *   Adds play/pause controls.
    *   On "Analyze" button click:
        *   Sends the audio file to the `/predict` endpoint of the Flask backend using `fetch`.
        *   Handles the JSON response containing predictions.
        *   Displays prediction results (textually and potentially overlaying regions on the waveform via WaveSurfer Regions plugin - **Note:** Requires integrating the plugin).
    *   Fetches dataset statistics from the `/stats` endpoint.
    *   Uses Chart.js to render the fetched statistics as bar charts.
*   **`templates/index.html`**: The main HTML structure of the web application.
    *   Includes elements for file upload, buttons, waveform display (`<div id="waveform">`), prediction results area, and canvas elements for charts.
    *   Links the `style.css` stylesheet and the `main.js` script.
    *   Includes necessary CDNs or local paths for libraries like WaveSurfer.js and Chart.js.
*   **`app.py`**: The Flask backend application.
    *   **Function:**
        1.  Initializes the Flask app.
        2.  Loads the trained model (`best_model.keras`) and the scaler (`scaler.joblib`) from `saved_assets/` during startup.
        3.  Defines the `/predict` route (endpoint):
            *   Accepts POST requests with an audio file.
            *   Loads the uploaded audio using Librosa.
            *   Applies sliding window analysis: segments the audio into overlapping windows.
            *   For each window: extracts MFCCs, pads/truncates, scales using the loaded scaler, and gets predictions from the loaded model.
            *   Formats predictions (start time, end time, crackle probability, wheeze probability) into a JSON response.
        4.  Defines the `/stats` route (endpoint):
            *   Provides pre-calculated or loaded statistics about the original dataset (e.g., abnormality prevalence by location/equipment) as a JSON response.
        5.  Defines the main route (`/`) to render the `index.html` template.
    *   **Runs:** Starts the development web server.
*   **`requirements.txt`**: Lists all Python packages required for the project (e.g., `Flask`, `tensorflow`, `librosa`, `numpy`, `pandas`, `scikit-learn`, `joblib`, `soundfile`). Enables easy installation using `pip install -r requirements.txt`.
*   **`README.md`**: Provides documentation for the project.

## Setup and Installation

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd abnormal-respiratory-sound-detector
    ```
2.  **Create and Activate Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    # On Windows:
    .\venv\Scripts\activate
    # On macOS/Linux:
    source venv/bin/activate
    ```
3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Place Raw Data:**
    *   Ensure the `audio_and_txt_files/` directory containing all `.wav` and `.txt` files is placed inside `data/raw/`.
    *   Ensure the `train_test.txt` file is placed inside `data/raw/`.

## Running the Project

**Important:** Execute these steps in order.

1.  **Prepare Data:** Run the data preparation script. This only needs to be done once unless the raw data changes.
    ```bash
    python scripts/prepare_data.py
    ```
    *   This will generate `saved_assets/scaler.joblib` and potentially `saved_assets/preprocessed_data.npz`.

2.  **Train Model:** Run the model training script. This reads the processed data (or requires `prepare_data.py` outputs) and trains the LSTM.
    ```bash
    python scripts/train_model.py
    ```
    *   This will generate `saved_assets/best_model.keras` (overwriting if it exists).

3.  **Run Backend Server:** Start the Flask application.
    ```bash
    flask run
    # OR, if flask run doesn't work directly:
    # python app.py
    ```
    *   The server will start, usually on `http://127.0.0.1:5000/`. It will load the model and scaler.

4.  **Access Frontend:** Open your web browser and navigate to the address provided by Flask (e.g., `http://127.0.0.1:5000/`).

## Usage

1.  Click the "Choose File" button to select an audio file from your computer.
2.  Once a file is selected and the waveform appears, click the "Analyze Audio" button.
3.  Wait for the processing to complete (a loading indicator may appear).
4.  Prediction results will be displayed below the waveform, showing probabilities for crackles and wheezes for different time segments.
5.  If the WaveSurfer Regions plugin is integrated, predicted segments will be highlighted on the waveform.
6.  Examine the "Dataset Insights" charts for context on abnormality prevalence in the original dataset.

## Notes

*   The performance of the model heavily depends on the quality of the data, feature extraction parameters (`N_MFCC`, `MAX_SEQ_LENGTH`, etc.), and the LSTM model architecture/hyperparameters.
*   The frontend visualization of predictions on the waveform requires integrating the `WaveSurfer.js Regions Plugin`. The provided `main.js` has placeholder comments where this integration would occur.
*   Ensure constants like `TARGET_SR`, `N_MFCC`, `MAX_SEQ_LENGTH`, `N_FFT`, `HOP_LENGTH` are consistent across `prepare_data.py`, `train_model.py`, and `app.py`.